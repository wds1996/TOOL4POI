{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30a9058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建 poi_info.csv 文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datafold = 'CA' \n",
    "# 读取 data.csv 文件\n",
    "try:\n",
    "    df = pd.read_csv(f'{datafold}/{datafold}.csv')\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：{datafold}.csv 文件未找到。请确保文件位于正确的目录下。\")\n",
    "    exit()\n",
    "\n",
    "# 按照 'pid' 进行分组\n",
    "grouped = df.groupby('pid')\n",
    "\n",
    "# 创建 poi_info 数据\n",
    "poi_info_data = []\n",
    "for pid, group in grouped:\n",
    "    # pid = int(pid)\n",
    "    category = group['category'].iloc[0]\n",
    "    region = group['region'].iloc[0]\n",
    "    latitude, longitude = group[['latitude', 'longitude']].iloc[0]\n",
    "    hourly_visits = {}\n",
    "    for timestamp_str in group['time']:\n",
    "        try:\n",
    "            hour = pd.to_datetime(timestamp_str).hour\n",
    "            hourly_visits[hour] = hourly_visits.get(hour, 0) + 1\n",
    "        except ValueError:\n",
    "            print(f\"警告：无法解析时间戳：{timestamp_str}。已跳过。\")\n",
    "    \n",
    "    # 只保留访问频率大于1的时间段\n",
    "    filtered_hourly_visits = {hour: count for hour, count in hourly_visits.items() if count > 1}\n",
    "    # 按访问次数（值）降序排序\n",
    "    sorted_hourly_visits = dict(\n",
    "        sorted(filtered_hourly_visits.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    poi_info_data.append({\n",
    "        'pid': pid,\n",
    "        'category': category,\n",
    "        'region': region,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'visit_time_and_count': sorted_hourly_visits\n",
    "    })\n",
    "\n",
    "# 创建 poi_info DataFrame\n",
    "poi_info_df = pd.DataFrame(poi_info_data)\n",
    "\n",
    "# 将 poi_info DataFrame 保存到 poi_info.csv 文件\n",
    "poi_info_df.to_csv(f'{datafold}/poi_info.csv', index=False)\n",
    "\n",
    "print(\"成功创建 poi_info.csv 文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fb500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distances (parallel): 100%|██████████| 5135/5135 [01:12<00:00, 70.72POI/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix calculated and saved to distance.csv (parallel, 2 decimal places)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "datafold = 'CA'\n",
    "\n",
    "# 加载 POI 数据\n",
    "try:\n",
    "    poi_df = pd.read_csv(f'{datafold}/poi_info.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: poi_info.csv not found. Please ensure it's in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "coords = poi_df[['latitude', 'longitude']].to_numpy()\n",
    "poi_ids = poi_df['pid'].tolist()\n",
    "num_pois = len(poi_df)\n",
    "\n",
    "def calculate_distances(i):\n",
    "    distances = {}\n",
    "    for j in range(num_pois):\n",
    "        if i == j:\n",
    "            distances[poi_ids[j]] = 0.0\n",
    "        else:\n",
    "            distance = geodesic(coords[i], coords[j]).km\n",
    "            distances[poi_ids[j]] = round(distance, 2)  # 保留两位小数\n",
    "    return poi_ids[i], distances\n",
    "\n",
    "\n",
    "num_processes = 128  # 根据你的 CPU 核心数调整\n",
    "pool = Pool(processes=num_processes)\n",
    "results = []\n",
    "with tqdm(total=num_pois, desc=\"Calculating distances (parallel)\", unit=\"POI\") as pbar:\n",
    "    for pid, distances in pool.imap_unordered(calculate_distances, range(num_pois)):\n",
    "        results.append((pid, distances))\n",
    "        pbar.update(1)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# 将结果转换为 DataFrame\n",
    "distance_matrix_data = {}\n",
    "for pid, distances in results:\n",
    "    distance_matrix_data[pid] = distances\n",
    "\n",
    "distance_matrix_df = pd.DataFrame.from_dict(distance_matrix_data, orient='index', columns=poi_ids)\n",
    "\n",
    "# 确保列的顺序与索引一致\n",
    "distance_matrix_df = distance_matrix_df.reindex(columns=poi_ids)\n",
    "\n",
    "# 保存距离矩阵到 CSV 文件\n",
    "distance_matrix_df.to_csv(f'{datafold}/distance.csv')\n",
    "\n",
    "print(\"Distance matrix calculated and saved to distance.csv (parallel, 2 decimal places)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d56e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离 PID 0 小于 2.0 的 POI PID 有723个:\n",
      "['0', '8', '20', '28', '62', '76', '77', '83', '85', '88', '91', '112', '119', '120', '121', '132', '134', '146', '152', '153', '181', '186', '194', '197', '206', '207', '208', '220', '222', '234', '237', '247', '252', '253', '256', '259', '260', '277', '288', '289', '292', '310', '311', '318', '319', '339', '344', '356', '363', '365', '366', '370', '371', '375', '383', '385', '387', '391', '395', '400', '401', '410', '414', '417', '421', '427', '428', '458', '460', '470', '493', '494', '512', '515', '516', '530', '537', '545', '552', '563', '574', '580', '651', '652', '664', '674', '675', '686', '689', '708', '732', '760', '763', '789', '799', '810', '817', '821', '831', '845', '848', '851', '854', '857', '858', '871', '875', '876', '882', '890', '905', '910', '912', '915', '918', '926', '939', '956', '960', '966', '975', '978', '983', '986', '991', '994', '1002', '1004', '1020', '1021', '1022', '1025', '1038', '1048', '1067', '1068', '1069', '1074', '1086', '1090', '1100', '1104', '1109', '1112', '1115', '1116', '1118', '1119', '1129', '1140', '1145', '1151', '1153', '1154', '1155', '1161', '1168', '1172', '1175', '1178', '1179', '1188', '1192', '1196', '1198', '1200', '1201', '1202', '1204', '1222', '1227', '1229', '1233', '1234', '1235', '1270', '1279', '1280', '1285', '1290', '1291', '1292', '1306', '1312', '1316', '1319', '1322', '1332', '1333', '1340', '1341', '1343', '1346', '1382', '1383', '1385', '1396', '1417', '1458', '1488', '1509', '1512', '1520', '1528', '1530', '1533', '1534', '1537', '1546', '1553', '1555', '1556', '1557', '1563', '1570', '1572', '1578', '1589', '1596', '1599', '1602', '1627', '1635', '1649', '1659', '1662', '1664', '1670', '1671', '1679', '1680', '1701', '1720', '1726', '1731', '1736', '1739', '1745', '1747', '1752', '1754', '1761', '1764', '1767', '1776', '1777', '1778', '1779', '1784', '1786', '1807', '1819', '1823', '1832', '1848', '1881', '1885', '1887', '1888', '1894', '1903', '1904', '1912', '1913', '1923', '1928', '1941', '1945', '1947', '1965', '1967', '1974', '1978', '1979', '1981', '1990', '1995', '2022', '2045', '2046', '2049', '2062', '2065', '2076', '2077', '2085', '2110', '2117', '2139', '2152', '2164', '2171', '2176', '2180', '2183', '2195', '2196', '2213', '2214', '2232', '2234', '2272', '2274', '2288', '2291', '2294', '2306', '2311', '2315', '2318', '2320', '2326', '2334', '2346', '2354', '2357', '2359', '2384', '2387', '2388', '2391', '2399', '2400', '2410', '2418', '2422', '2424', '2442', '2447', '2449', '2451', '2452', '2453', '2455', '2457', '2468', '2477', '2478', '2484', '2489', '2494', '2497', '2503', '2506', '2510', '2519', '2521', '2527', '2537', '2540', '2541', '2545', '2551', '2552', '2564', '2570', '2576', '2577', '2589', '2595', '2613', '2617', '2627', '2630', '2644', '2645', '2667', '2668', '2669', '2673', '2678', '2679', '2683', '2686', '2690', '2694', '2695', '2707', '2708', '2711', '2714', '2715', '2726', '2729', '2743', '2747', '2749', '2756', '2761', '2765', '2790', '2805', '2818', '2840', '2851', '2856', '2859', '2863', '2865', '2866', '2869', '2873', '2877', '2878', '2883', '2888', '2894', '2897', '2898', '2905', '2909', '2913', '2914', '2918', '2926', '2939', '2940', '2955', '2961', '2964', '2971', '2972', '2982', '3023', '3026', '3029', '3036', '3038', '3043', '3045', '3046', '3048', '3068', '3077', '3078', '3079', '3080', '3082', '3088', '3090', '3094', '3097', '3098', '3121', '3131', '3132', '3135', '3143', '3147', '3148', '3151', '3156', '3177', '3179', '3195', '3207', '3208', '3220', '3222', '3224', '3231', '3232', '3233', '3235', '3241', '3246', '3248', '3249', '3254', '3272', '3274', '3284', '3286', '3291', '3295', '3296', '3297', '3304', '3308', '3319', '3320', '3322', '3326', '3330', '3338', '3340', '3342', '3346', '3350', '3364', '3365', '3366', '3368', '3371', '3377', '3378', '3396', '3398', '3401', '3407', '3409', '3416', '3418', '3428', '3434', '3435', '3444', '3468', '3475', '3482', '3494', '3499', '3507', '3511', '3517', '3533', '3537', '3539', '3549', '3551', '3562', '3567', '3569', '3580', '3595', '3597', '3598', '3600', '3606', '3607', '3615', '3621', '3633', '3640', '3650', '3656', '3660', '3666', '3669', '3670', '3672', '3677', '3679', '3682', '3686', '3691', '3705', '3709', '3714', '3716', '3722', '3728', '3730', '3745', '3757', '3760', '3761', '3770', '3771', '3774', '3777', '3779', '3782', '3786', '3788', '3791', '3793', '3797', '3798', '3804', '3824', '3832', '3850', '3853', '3855', '3859', '3860', '3864', '3865', '3869', '3873', '3876', '3887', '3902', '3905', '3925', '3937', '3951', '3954', '3964', '3966', '3969', '3973', '3975', '3982', '3984', '3988', '3992', '3993', '4004', '4006', '4015', '4023', '4031', '4052', '4060', '4089', '4092', '4097', '4100', '4102', '4106', '4114', '4122', '4123', '4134', '4135', '4151', '4152', '4154', '4165', '4176', '4177', '4183', '4185', '4191', '4198', '4201', '4210', '4212', '4232', '4254', '4260', '4261', '4264', '4266', '4267', '4269', '4286', '4289', '4291', '4303', '4307', '4316', '4319', '4321', '4322', '4325', '4335', '4349', '4352', '4378', '4383', '4398', '4399', '4401', '4407', '4411', '4422', '4431', '4433', '4457', '4481', '4509', '4513', '4514', '4515', '4516', '4520', '4527', '4535', '4539', '4548', '4549', '4553', '4561', '4582', '4605', '4611', '4634', '4640', '4643', '4657', '4664', '4684', '4688', '4714', '4757', '4762', '4764', '4776', '4785', '4797', '4862', '4866', '4873', '4874', '4876', '4881', '4882', '4904', '4912', '4913', '4915', '4918', '4926', '4927', '4929', '4936', '4937', '4944', '4950', '4953', '4962', '4967', '4973', '4994', '5002', '5009', '5051', '5091', '5107', '5117']\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def find_nearby_pois(distance_file, target_pid, threshold):\n",
    "#     \"\"\"\n",
    "#     查找距离指定 PID 小于阈值的其他 POI 的 PID。\n",
    "\n",
    "#     Args:\n",
    "#         distance_file (str): 存储 POI 之间距离的 CSV 文件路径。\n",
    "#         target_pid (int): 要查找附近 POI 的目标 POI 的 PID。\n",
    "#         threshold (float): 距离阈值（单位与 distance_file 中的距离单位一致）。\n",
    "\n",
    "#     Returns:\n",
    "#         list: 包含所有距离目标 PID 小于阈值的其他 POI 的 PID 列表。\n",
    "#               如果目标 PID 不存在于文件中，则返回一个空列表。\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         distance_df = pd.read_csv(distance_file, index_col=0)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"错误：文件 {distance_file} 未找到。\")\n",
    "#         return []\n",
    "\n",
    "#     if target_pid not in distance_df.index:\n",
    "#         print(f\"警告：目标 PID {target_pid} 不存在于距离文件中。\")\n",
    "#         return []\n",
    "\n",
    "#     # 获取目标 PID 对应的距离Series\n",
    "#     target_distances = distance_df.loc[target_pid]\n",
    "\n",
    "#     # 筛选出距离小于阈值的 PID，并排除目标 PID 本身\n",
    "#     nearby_pois = target_distances[\n",
    "#         (target_distances < threshold) & (distance_df.index != target_pid)\n",
    "#     ].index.tolist()\n",
    "\n",
    "#     return nearby_pois\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     distance_file = 'NYC/distance.csv'  # 替换为你的 distance.csv 文件路径\n",
    "#     target_pid = 0  # 指定你要查找附近 POI 的目标 PID\n",
    "#     distance_threshold = 2.0  # 指定距离阈值，例如 1.0 公里\n",
    "\n",
    "#     nearby_pids = find_nearby_pois(distance_file, target_pid, distance_threshold)\n",
    "\n",
    "#     if nearby_pids:\n",
    "#         print(f\"距离 PID {target_pid} 小于 {distance_threshold} 的 POI PID 有{len(nearby_pids)}个:\")\n",
    "#         print(nearby_pids)\n",
    "#     else:\n",
    "#         print(f\"没有找到距离 PID {target_pid} 小于 {distance_threshold} 的其他 POI。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb33695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1654516/1377472950.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到：CA/poi_checkin.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "\n",
    "def process_poi_data(file_path):\n",
    "    \"\"\"\n",
    "    处理POI签到数据，按用户ID分组，并按时间排序POI访问记录。\n",
    "    计算每个POI与前一个POI的距离。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含POI签到数据的文件的路径。\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame: 包含处理后数据的DataFrame，其中包含uid、pid_list、category_list、time_list和distance_list列。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 确保时间列是datetime类型\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # 按uid分组并聚合数据\n",
    "    def aggregate_and_calculate_distance(group):\n",
    "        pid_list = list(group['pid'])\n",
    "        category_list = list(group['category'])\n",
    "        region_list = list(group['region'])\n",
    "        time_list = list(group['time'])\n",
    "        \n",
    "        # 按时间排序\n",
    "        sorted_indices = sorted(range(len(time_list)), key=lambda i: time_list[i])\n",
    "        pid_list = [pid_list[i] for i in sorted_indices]\n",
    "        category_list = [category_list[i] for i in sorted_indices]\n",
    "        region_list = [region_list[i] for i in sorted_indices]\n",
    "        time_list = [time_list[i] for i in sorted_indices]\n",
    "        \n",
    "        # 计算距离\n",
    "        # distance_list = [0.0]  # 第一个POI的距离为0\n",
    "        # for i in range(1, len(latitude_list)):\n",
    "        #     coord1 = (latitude_list[i-1], longitude_list[i-1])\n",
    "        #     coord2 = (latitude_list[i], longitude_list[i])\n",
    "        #     distance = geodesic(coord1, coord2).km  # 使用geodesic计算距离，单位为公里\n",
    "        #     distance_list.append(round(distance, 2))\n",
    "        \n",
    "        return pd.Series({\n",
    "            'pid_list': pid_list,\n",
    "            'category_list': category_list,\n",
    "            'region_list': region_list,\n",
    "            'time_list': [t.strftime('%Y-%m-%d %H:%M') for t in time_list], # 格式化时间\n",
    "            # 'distance_list': distance_list\n",
    "        })\n",
    "\n",
    "    df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "def save_processed_data(df, output_file_path):\n",
    "    \"\"\"\n",
    "    将处理后的数据保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    df (pandas.DataFrame): 包含处理后数据的DataFrame。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    df.to_csv(output_file_path, index=False, header=True)\n",
    "    print(f\"处理后的数据已保存到：{output_file_path}\")\n",
    "\n",
    "# 指定输入和输出文件路径\n",
    "input_file_path = f'{datafold}/{datafold}.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "# 处理数据\n",
    "processed_df = process_poi_data(input_file_path)\n",
    "\n",
    "# 保存处理后的数据\n",
    "save_processed_data(processed_df, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f85a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移图已保存到：CA/poi_transition_graph.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "\n",
    "def build_poi_transition_graph(file_path):\n",
    "    \"\"\"\n",
    "    根据用户访问序列文件构建POI到POI的转移图。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含用户访问序列数据的文件路径。\n",
    "\n",
    "    返回:\n",
    "    dict: 一个字典，表示POI到POI的转移图。\n",
    "          键是POI ID，值是潜在的后续POI ID列表。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 初始化转移图\n",
    "    transition_graph = defaultdict(list)\n",
    "\n",
    "    df['poi_list'] = df['pid_list'].apply(lambda x: eval(x))  # 将字符串转换为列表\n",
    "    user_checkin = df['poi_list'].tolist()\n",
    "\n",
    "    for i in range(len(user_checkin)):\n",
    "        # 获取当前用户的POI列表\n",
    "        poi_list = user_checkin[i]\n",
    "        # 遍历当前用户访问的POI列表\n",
    "        for j in range(len(poi_list) - 1):\n",
    "            current_poi = poi_list[j]\n",
    "            next_poi = poi_list[j + 1]\n",
    "            # 将下一个POI添加到当前POI的潜在后续POI列表中\n",
    "            if next_poi not in transition_graph[current_poi]:\n",
    "                transition_graph[current_poi].append(next_poi)\n",
    "\n",
    "    return transition_graph\n",
    "\n",
    "def save_transition_graph(graph, output_file_path):\n",
    "    \"\"\"\n",
    "    将POI到POI的转移图保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    graph (dict): POI到POI的转移图。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    # 将字典转换为DataFrame\n",
    "    df = pd.DataFrame(list(graph.items()), columns=['pid', 'potential_poi'])\n",
    "    # 保存到CSV文件\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"转移图已保存到：{output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入和输出文件路径\n",
    "    input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "    output_file_path = f'{datafold}/poi_transition_graph.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "    # 构建转移图\n",
    "    transition_graph = build_poi_transition_graph(input_file_path)\n",
    "\n",
    "    # 保存转移图\n",
    "    save_transition_graph(transition_graph, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91232bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: NYC/data.json\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# import ast\n",
    "\n",
    "\n",
    "# datafold = 'NYC'\n",
    "# def convert_csv_to_json(input_csv_path, output_json_path):\n",
    "#     data = []\n",
    "\n",
    "#     with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             uid = int(row['uid'])\n",
    "#             pid_list = ast.literal_eval(row['pid_list'])\n",
    "#             category_list = ast.literal_eval(row['category_list'])\n",
    "#             region_list = ast.literal_eval(row['region_list'])\n",
    "#             time_list = ast.literal_eval(row['time_list'])\n",
    "#             # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            \n",
    "#             max_length = 50\n",
    "#             record = {\n",
    "#                 \"uid\": uid,\n",
    "#                 \"pid_list\": pid_list[-max_length:] if len(pid_list) > max_length else pid_list,\n",
    "#                 \"category_list\": category_list[-max_length:] if len(category_list) > max_length else category_list,\n",
    "#                 \"region_list\": region_list[-max_length:] if len(region_list) > max_length else region_list,\n",
    "#                 \"time_list\": time_list[-max_length:] if len(time_list) > max_length else time_list,\n",
    "#                 # \"distance_list\": distance_list[-max_length:] if len(distance_list) > max_length else distance_list\n",
    "#             }\n",
    "\n",
    "#             data.append(record)\n",
    "\n",
    "#     with open(output_json_path, mode='w', encoding='utf-8') as jsonfile:\n",
    "#         jsonfile.write('[\\n')\n",
    "#         for i, record in enumerate(data):\n",
    "#             line = json.dumps(record, ensure_ascii=False, separators=(',', ': '))\n",
    "#             jsonfile.write('  ' + line)\n",
    "#             if i < len(data) - 1:\n",
    "#                 jsonfile.write(',\\n')\n",
    "#             else:\n",
    "#                 jsonfile.write('\\n')\n",
    "#         jsonfile.write(']\\n')\n",
    "\n",
    "#     print(f\"转换完成，结果保存至: {output_json_path}\")\n",
    "# # 示例用法\n",
    "# input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "# output_file_path = f'{datafold}/data.json'  # 替换为您想要的输出文件路径\n",
    "# convert_csv_to_json(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b03882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/data50.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "max_length = 50\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            # category_list = ast.literal_eval(row['category_list'])\n",
    "            # region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"history\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list,\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list,\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        history = record[\"history\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "            # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The historical POI check-in records for user {uid} are as follows:\\n\"\n",
    "            f\"POI list: {history}, with corresponding check-in times: {time_seq}.\"\n",
    "            # f\"At {next_time}, which POI is the user most likely to check in at?\"\n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/data{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e856aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376af77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: TKY/data100.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "max_length = 100\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The historical POI check-in records: {pid_list}.\" \n",
    "            # f\"with corresponding categories: {category},\\n\"\n",
    "            # f\"regions: {region},\\n\"\n",
    "            # f\"with corresponding check-in times: {time_seq}.\"\n",
    "            # f\"At {next_time}, which POI is the user most likely to check in at?\"\n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            # \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/data{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8b9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: TKY/data50.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'TKY'\n",
    "max_length = 50\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        # input_text = (\n",
    "        #     f\"The user has recently visited the following: {pid_list}, with corresponding check-in times: {time_seq}.\"\n",
    "        # )\n",
    "\n",
    "        sequence = [\n",
    "            f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}, ' if i < len(pid_list) - 1 else\n",
    "            f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}.'\n",
    "            for i, poi in enumerate(pid_list)\n",
    "        ]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = f\"User_{uid} visited: \" + \"\".join(sequence) + f\" Now is {next_time}, user_{uid} is likely to visit?\"\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            # \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/data{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363eaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc15741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a1d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "datafold = 'NYC'\n",
    "file_name = f\"{datafold}/{datafold}.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df = df[['uid', 'pid', 'category', 'region', 'latitude', 'longitude', 'time']]\n",
    "# 按照时间排序\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# 计算80%数据的索引\n",
    "train_size = int(0.8 * len(df))\n",
    "\n",
    "# 将前80%作为训练集\n",
    "train_df = df[:train_size]\n",
    "# 将后20%作为测试集\n",
    "test_df = df[train_size:]\n",
    "\n",
    "def romove_users_pois_test(df_train, df_test):\n",
    "    users_train = df_train['uid'].unique()\n",
    "    df_test = df_test[df_test['uid'].isin(users_train)]\n",
    "    users_test = df_test['uid'].unique()\n",
    "    df_train = df_train[df_train['uid'].isin(users_test)]\n",
    "\n",
    "    pois_train = df_train['pid'].unique()\n",
    "    df_test = df_test[df_test['pid'].isin(pois_train)]\n",
    "    return df_test\n",
    "\n",
    "test_df = romove_users_pois_test(train_df, test_df)\n",
    "\n",
    "# 将训练集和测试集合并\n",
    "new_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# 获取测试集中所有需要保留的 uid\n",
    "test_uids = test_df['uid'].unique()\n",
    "\n",
    "# 过滤原始 df，只保留那些 Uid 出现在 test_df 中的记录\n",
    "expanded_df = new_df[new_df['uid'].isin(test_uids)]\n",
    "\n",
    "\n",
    "\n",
    "# 保存训练集和测试集\n",
    "# train_df.to_csv(f'{datafold}/train_data.csv', index=False)\n",
    "expanded_df.to_csv(f'{datafold}/my_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3421fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建 poi_info.csv 文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datafold = 'NYC' \n",
    "# 读取 data.csv 文件\n",
    "try:\n",
    "    df = pd.read_csv(f'{datafold}/my_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：{datafold}.csv 文件未找到。请确保文件位于正确的目录下。\")\n",
    "    exit()\n",
    "\n",
    "# 按照 'pid' 进行分组\n",
    "grouped = df.groupby('pid')\n",
    "\n",
    "# 创建 poi_info 数据\n",
    "poi_info_data = []\n",
    "for pid, group in grouped:\n",
    "    # pid = int(pid)\n",
    "    category = group['category'].iloc[0]\n",
    "    region = group['region'].iloc[0]\n",
    "    latitude, longitude = group[['latitude', 'longitude']].iloc[0]\n",
    "    hourly_visits = {}\n",
    "    for timestamp_str in group['time']:\n",
    "        try:\n",
    "            hour = pd.to_datetime(timestamp_str).hour\n",
    "            hourly_visits[hour] = hourly_visits.get(hour, 0) + 1\n",
    "        except ValueError:\n",
    "            print(f\"警告：无法解析时间戳：{timestamp_str}。已跳过。\")\n",
    "    \n",
    "    # 只保留访问频率大于1的时间段\n",
    "    filtered_hourly_visits = {hour: count for hour, count in hourly_visits.items() if count > 1}\n",
    "    # 按访问次数（值）降序排序\n",
    "    sorted_hourly_visits = dict(\n",
    "        sorted(filtered_hourly_visits.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    poi_info_data.append({\n",
    "        'pid': pid,\n",
    "        'category': category,\n",
    "        'region': region,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'visit_time_and_count': sorted_hourly_visits\n",
    "    })\n",
    "\n",
    "# 创建 poi_info DataFrame\n",
    "poi_info_df = pd.DataFrame(poi_info_data)\n",
    "\n",
    "# 将 poi_info DataFrame 保存到 poi_info.csv 文件\n",
    "poi_info_df.to_csv(f'{datafold}/poi_info.csv', index=False)\n",
    "\n",
    "print(\"成功创建 poi_info.csv 文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459a0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到：NYC/poi_checkin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156033/1363854695.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "datafold = 'NYC'\n",
    "\n",
    "def process_poi_data(file_path, max_length=50):\n",
    "    \"\"\"\n",
    "    处理POI签到数据，按用户ID分组，并按时间排序POI访问记录。\n",
    "    计算每个POI与前一个POI的距离。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含POI签到数据的文件的路径。\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame: 包含处理后数据的DataFrame，其中包含uid、pid_list、category_list、time_list和distance_list列。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 确保时间列是datetime类型\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # 按uid分组并聚合数据\n",
    "    def aggregate_and_calculate_distance(group):\n",
    "        pid_list = list(group['pid'])\n",
    "        category_list = list(group['category'])\n",
    "        region_list = list(group['region'])\n",
    "        time_list = list(group['time'])\n",
    "        \n",
    "        # 按时间排序\n",
    "        sorted_indices = sorted(range(len(time_list)), key=lambda i: time_list[i])\n",
    "        pid_list = [pid_list[i] for i in sorted_indices]\n",
    "        category_list = [category_list[i] for i in sorted_indices]\n",
    "        region_list = [region_list[i] for i in sorted_indices]\n",
    "        time_list = [time_list[i] for i in sorted_indices]\n",
    "        \n",
    "        pid_list = pid_list[:max_length] if len(pid_list) > max_length else pid_list\n",
    "        category_list = category_list[:max_length] if len(category_list) > max_length else category_list\n",
    "        region_list = region_list[:max_length] if len(region_list) > max_length else region_list\n",
    "        time_list = time_list[:max_length] if len(time_list) > max_length else time_list\n",
    "\n",
    "        return pd.Series({\n",
    "            'pid_list': pid_list,\n",
    "            'category_list': category_list,\n",
    "            'region_list': region_list,\n",
    "            'time_list': [t.strftime('%Y-%m-%d %H:%M') for t in time_list], # 格式化时间\n",
    "            # 'distance_list': distance_list\n",
    "        })\n",
    "\n",
    "    df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "def save_processed_data(df, output_file_path):\n",
    "    \"\"\"\n",
    "    将处理后的数据保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    df (pandas.DataFrame): 包含处理后数据的DataFrame。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    df.to_csv(output_file_path, index=False, header=True)\n",
    "    print(f\"处理后的数据已保存到：{output_file_path}\")\n",
    "\n",
    "# 指定输入和输出文件路径\n",
    "input_file_path = f'{datafold}/my_data.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您想要的输出文件路径\n",
    "max_length = 100\n",
    "# 处理数据\n",
    "processed_df = process_poi_data(input_file_path, max_length)\n",
    "\n",
    "# 保存处理后的数据\n",
    "save_processed_data(processed_df, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8e2c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移图已保存到：CA/poi_transition_graph.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "datafold = 'CA' # NYC, TKY, CA\n",
    "\n",
    "def build_poi_transition_graph(file_path):\n",
    "    \"\"\"\n",
    "    根据用户访问序列文件构建POI到POI的转移图。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含用户访问序列数据的文件路径。\n",
    "\n",
    "    返回:\n",
    "    dict: 一个字典，表示POI到POI的转移图。\n",
    "          键是POI ID，值是潜在的后续POI ID列表。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 初始化转移图\n",
    "    transition_graph = defaultdict(list)\n",
    "\n",
    "    df['poi_list'] = df['pid_list'].apply(lambda x: eval(x))  # 将字符串转换为列表\n",
    "    user_checkin = df['poi_list'].tolist()\n",
    "\n",
    "    # 数据-1 防止数据泄漏\n",
    "    user_checkin = user_checkin[:-1]  # 去掉最后一个用户的签到记录\n",
    "    for i in range(len(user_checkin)):\n",
    "        # 获取当前用户的POI列表\n",
    "        poi_list = user_checkin[i]\n",
    "        # 遍历当前用户访问的POI列表\n",
    "        for j in range(len(poi_list) - 1):\n",
    "            current_poi = poi_list[j]\n",
    "            next_poi = poi_list[j + 1]\n",
    "            # 将下一个POI添加到当前POI的潜在后续POI列表中\n",
    "            if next_poi not in transition_graph[current_poi]:\n",
    "                transition_graph[current_poi].append(next_poi)\n",
    "\n",
    "    return transition_graph\n",
    "\n",
    "def save_transition_graph(graph, output_file_path):\n",
    "    \"\"\"\n",
    "    将POI到POI的转移图保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    graph (dict): POI到POI的转移图。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    # 将字典转换为DataFrame\n",
    "    df = pd.DataFrame(list(graph.items()), columns=['pid', 'potential_poi'])\n",
    "    # 保存到CSV文件\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"转移图已保存到：{output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入和输出文件路径\n",
    "    input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "    output_file_path = f'{datafold}/poi_transition_graph.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "    # 构建转移图\n",
    "    transition_graph = build_poi_transition_graph(input_file_path)\n",
    "\n",
    "    # 保存转移图\n",
    "    save_transition_graph(transition_graph, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "076ccca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/history100.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "max_length = 100\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The historical POI check-in records: {pid_list}.\" \n",
    "            # f\"with corresponding categories: {category},\\n\"\n",
    "            # f\"regions: {region},\\n\"\n",
    "            # f\"with corresponding check-in times: {time_seq}.\"\n",
    "            # f\"At {next_time}, which POI is the user most likely to check in at?\"\n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            # \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/history{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff65bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/recent50.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "max_length = 50\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The user{uid} has recently POI check-in records: {pid_list}, with corresponding check-in times: {time_seq}.\"\n",
    "        )\n",
    "\n",
    "        # sequence = [\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}, ' if i < len(pid_list) - 1 else\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}.'\n",
    "        #     for i, poi in enumerate(pid_list)\n",
    "        # ]\n",
    "\n",
    "        # # 构造 input 字符串\n",
    "        # input_text = f\"User_{uid} visited: \" + \"\".join(sequence) + f\" Now is {next_time}, user_{uid} is likely to visit?\"\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/recent{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f717151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/recent20.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'CA'\n",
    "max_length = 20\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The user{uid} has recently POI check-in records: {pid_list}, with corresponding check-in times: {time_seq}.\"\n",
    "        )\n",
    "\n",
    "        # sequence = [\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}, ' if i < len(pid_list) - 1 else\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}.'\n",
    "        #     for i, poi in enumerate(pid_list)\n",
    "        # ]\n",
    "\n",
    "        # # 构造 input 字符串\n",
    "        # input_text = f\"User_{uid} visited: \" + \"\".join(sequence) + f\" Now is {next_time}, user_{uid} is likely to visit?\"\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/recent{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToolRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
