{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "datafold = 'NYC'\n",
    "file_name = f\"{datafold}/{datafold}.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df = df[['uid', 'pid', 'category', 'region', 'latitude', 'longitude', 'time']]\n",
    "# 按照时间排序\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# 计算80%数据的索引\n",
    "train_size = int(0.8 * len(df))\n",
    "\n",
    "# 将前80%作为训练集\n",
    "train_df = df[:train_size]\n",
    "# 将后20%作为测试集\n",
    "test_df = df[train_size:]\n",
    "\n",
    "def romove_users_pois_test(df_train, df_test):\n",
    "    users_train = df_train['uid'].unique()\n",
    "    df_test = df_test[df_test['uid'].isin(users_train)]\n",
    "    users_test = df_test['uid'].unique()\n",
    "    df_train = df_train[df_train['uid'].isin(users_test)]\n",
    "\n",
    "    pois_train = df_train['pid'].unique()\n",
    "    df_test = df_test[df_test['pid'].isin(pois_train)]\n",
    "    return df_test\n",
    "\n",
    "test_df = romove_users_pois_test(train_df, test_df)\n",
    "\n",
    "# 将训练集和测试集合并\n",
    "new_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# 获取测试集中所有需要保留的 uid\n",
    "test_uids = test_df['uid'].unique()\n",
    "\n",
    "# 过滤原始 df，只保留那些 Uid 出现在 test_df 中的记录\n",
    "expanded_df = new_df[new_df['uid'].isin(test_uids)]\n",
    "\n",
    "\n",
    "\n",
    "# 保存训练集和测试集\n",
    "# train_df.to_csv(f'{datafold}/train_data.csv', index=False)\n",
    "expanded_df.to_csv(f'{datafold}/my_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3421fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建 poi_info.csv 文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datafold = 'NYC' \n",
    "# 读取 data.csv 文件\n",
    "try:\n",
    "    df = pd.read_csv(f'{datafold}/my_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：{datafold}.csv 文件未找到。请确保文件位于正确的目录下。\")\n",
    "    exit()\n",
    "\n",
    "# 按照 'pid' 进行分组\n",
    "grouped = df.groupby('pid')\n",
    "\n",
    "# 创建 poi_info 数据\n",
    "poi_info_data = []\n",
    "for pid, group in grouped:\n",
    "    # pid = int(pid)\n",
    "    category = group['category'].iloc[0]\n",
    "    region = group['region'].iloc[0]\n",
    "    latitude, longitude = group[['latitude', 'longitude']].iloc[0]\n",
    "    hourly_visits = {}\n",
    "    for timestamp_str in group['time']:\n",
    "        try:\n",
    "            hour = pd.to_datetime(timestamp_str).hour\n",
    "            hourly_visits[hour] = hourly_visits.get(hour, 0) + 1\n",
    "        except ValueError:\n",
    "            print(f\"警告：无法解析时间戳：{timestamp_str}。已跳过。\")\n",
    "    \n",
    "    # 只保留访问频率大于1的时间段\n",
    "    filtered_hourly_visits = {hour: count for hour, count in hourly_visits.items() if count > 1}\n",
    "    # 按访问次数（值）降序排序\n",
    "    sorted_hourly_visits = dict(\n",
    "        sorted(filtered_hourly_visits.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    poi_info_data.append({\n",
    "        'pid': pid,\n",
    "        'category': category,\n",
    "        'region': region,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'visit_time_and_count': sorted_hourly_visits\n",
    "    })\n",
    "\n",
    "# 创建 poi_info DataFrame\n",
    "poi_info_df = pd.DataFrame(poi_info_data)\n",
    "\n",
    "# 将 poi_info DataFrame 保存到 poi_info.csv 文件\n",
    "poi_info_df.to_csv(f'{datafold}/poi_info.csv', index=False)\n",
    "\n",
    "print(\"成功创建 poi_info.csv 文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到：NYC/poi_checkin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156033/1363854695.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "datafold = 'NYC'\n",
    "\n",
    "def process_poi_data(file_path, max_length=50):\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 确保时间列是datetime类型\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # 按uid分组并聚合数据\n",
    "    def aggregate_and_calculate_distance(group):\n",
    "        pid_list = list(group['pid'])\n",
    "        category_list = list(group['category'])\n",
    "        region_list = list(group['region'])\n",
    "        time_list = list(group['time'])\n",
    "        \n",
    "        # 按时间排序\n",
    "        sorted_indices = sorted(range(len(time_list)), key=lambda i: time_list[i])\n",
    "        pid_list = [pid_list[i] for i in sorted_indices]\n",
    "        category_list = [category_list[i] for i in sorted_indices]\n",
    "        region_list = [region_list[i] for i in sorted_indices]\n",
    "        time_list = [time_list[i] for i in sorted_indices]\n",
    "        \n",
    "        pid_list = pid_list[:max_length] if len(pid_list) > max_length else pid_list\n",
    "        category_list = category_list[:max_length] if len(category_list) > max_length else category_list\n",
    "        region_list = region_list[:max_length] if len(region_list) > max_length else region_list\n",
    "        time_list = time_list[:max_length] if len(time_list) > max_length else time_list\n",
    "\n",
    "        return pd.Series({\n",
    "            'pid_list': pid_list,\n",
    "            'category_list': category_list,\n",
    "            'region_list': region_list,\n",
    "            'time_list': [t.strftime('%Y-%m-%d %H:%M') for t in time_list], # 格式化时间\n",
    "            # 'distance_list': distance_list\n",
    "        })\n",
    "\n",
    "    df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "def save_processed_data(df, output_file_path):\n",
    "    df.to_csv(output_file_path, index=False, header=True)\n",
    "    print(f\"处理后的数据已保存到：{output_file_path}\")\n",
    "\n",
    "# 指定输入和输出文件路径\n",
    "input_file_path = f'{datafold}/my_data.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您想要的输出文件路径\n",
    "max_length = 100\n",
    "# 处理数据\n",
    "processed_df = process_poi_data(input_file_path, max_length)\n",
    "\n",
    "# 保存处理后的数据\n",
    "save_processed_data(processed_df, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移图已保存到：CA/poi_transition_graph.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "datafold = 'NYC' # NYC, TKY, CA\n",
    "\n",
    "def build_poi_transition_graph(file_path):\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 初始化转移图\n",
    "    transition_graph = defaultdict(list)\n",
    "\n",
    "    df['poi_list'] = df['pid_list'].apply(lambda x: eval(x))  # 将字符串转换为列表\n",
    "    user_checkin = df['poi_list'].tolist()\n",
    "\n",
    "    # 数据-1 防止数据泄漏\n",
    "    user_checkin = user_checkin[:-1]  # 去掉最后一个用户的签到记录\n",
    "    for i in range(len(user_checkin)):\n",
    "        # 获取当前用户的POI列表\n",
    "        poi_list = user_checkin[i]\n",
    "        # 遍历当前用户访问的POI列表\n",
    "        for j in range(len(poi_list) - 1):\n",
    "            current_poi = poi_list[j]\n",
    "            next_poi = poi_list[j + 1]\n",
    "            # 将下一个POI添加到当前POI的潜在后续POI列表中\n",
    "            if next_poi not in transition_graph[current_poi]:\n",
    "                transition_graph[current_poi].append(next_poi)\n",
    "\n",
    "    return transition_graph\n",
    "\n",
    "def save_transition_graph(graph, output_file_path):\n",
    "    # 将字典转换为DataFrame\n",
    "    df = pd.DataFrame(list(graph.items()), columns=['pid', 'potential_poi'])\n",
    "    # 保存到CSV文件\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"转移图已保存到：{output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入和输出文件路径\n",
    "    input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "    output_file_path = f'{datafold}/poi_transition_graph.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "    # 构建转移图\n",
    "    transition_graph = build_poi_transition_graph(input_file_path)\n",
    "\n",
    "    # 保存转移图\n",
    "    save_transition_graph(transition_graph, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ccca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/history100.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'NYC'\n",
    "max_length = 100\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The historical POI check-in records: {pid_list}.\" \n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            # \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/history{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff65bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/recent50.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'NYC'\n",
    "max_length = 50\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The user{uid} has recently POI check-in records: {pid_list}, with corresponding check-in times: {time_seq}.\"\n",
    "        )\n",
    "\n",
    "        # sequence = [\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}, ' if i < len(pid_list) - 1 else\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}.'\n",
    "        #     for i, poi in enumerate(pid_list)\n",
    "        # ]\n",
    "\n",
    "        # # 构造 input 字符串\n",
    "        # input_text = f\"User_{uid} visited: \" + \"\".join(sequence) + f\" Now is {next_time}, user_{uid} is likely to visit?\"\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/recent{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f717151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: CA/recent20.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'NYC'\n",
    "max_length = 20\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            category_list = ast.literal_eval(row['category_list'])\n",
    "            region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"pid_list\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list[:-1],\n",
    "                \"category\": category_list[-max_length-1:-1] if len(category_list) > max_length else category_list[:-1],\n",
    "                \"region\": region_list[-max_length-1:-1] if len(region_list) > max_length else region_list[:-1],\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list[:-1],\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        pid_list = record[\"pid_list\"]\n",
    "        category = record[\"category\"]\n",
    "        region = record[\"region\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "        # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The user{uid} has recently POI check-in records: {pid_list}, with corresponding check-in times: {time_seq}.\"\n",
    "        )\n",
    "\n",
    "        # sequence = [\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}, ' if i < len(pid_list) - 1 else\n",
    "        #     f\"poi {poi}\" + f' (belong to {category[i]}' + f', located in region {region[i]})' + f' at {time_seq[i]}.'\n",
    "        #     for i, poi in enumerate(pid_list)\n",
    "        # ]\n",
    "\n",
    "        # # 构造 input 字符串\n",
    "        # input_text = f\"User_{uid} visited: \" + \"\".join(sequence) + f\" Now is {next_time}, user_{uid} is likely to visit?\"\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/recent{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToolRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
